{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c350b4a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyspark in /usr/local/spark-3.2.4-bin-hadoop3.2/python (3.2.4)\n",
      "Collecting py4j==0.10.9.5\n",
      "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 KB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: py4j\n",
      "  Attempting uninstall: py4j\n",
      "    Found existing installation: py4j 0.10.9.7\n",
      "    Uninstalling py4j-0.10.9.7:\n",
      "      Successfully uninstalled py4j-0.10.9.7\n",
      "Successfully installed py4j-0.10.9.5\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59625b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51765d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "# Stop the existing SparkContext if any\n",
    "SparkContext.stop(sc)\n",
    "\n",
    "# Initialize a new SparkContext\n",
    "sc = SparkContext(master=\"local\", appName=\"New App\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21ad2ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Customer Reviews Analysis\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b26b6482",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "test_data = spark.read.csv(\"hdfs://localhost:9000/user/hduser/CustomerReviews/test/test.csv\", header=True, inferSchema=True)\n",
    "train_data = spark.read.csv(\"hdfs://localhost:9000/user/hduser/CustomerReviews/train/train.csv\", header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d859d760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|  1|     mens ultrasheer|This model may be ok for sedentary types, but I'm active and get around alot in my job - consistently found these stockings rolled up down by my ankles! Not Good!! Solution: go with the standard compression stocking, 20-30, stock #114622. Excellent support, stays up and gives me what I need. Both pair of these also tore as I struggled to pull them up all the time. Good riddance/bad investment!|\n",
      "+---+--------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|  4|Surprisingly deli...|                                                                                                                                                                                                                                                                                                                                                                                        This is a fast re...|\n",
      "|  2|Works, but not as...|                                                                                                                                                                                                                                                                                                                                                                                        \"I bought one of ...|\n",
      "|  2|             Oh dear|                                                                                                                                                                                                                                                                                                                                                                                        \"I was excited to...|\n",
      "|  2|     Incorrect disc!|                                                                                                                                                                                                                                                                                                                                                                                        \"I am a big JVC f...|\n",
      "|  2|      Incorrect Disc|                                                                                                                                                                                                                                                                                                                                                                                        I love the style ...|\n",
      "|  2|DVD menu select p...|                                                                                                                                                                                                                                                                                                                                                                                        I cannot scroll t...|\n",
      "|  3|My 2 y/o grandson...|                                                                                                                                                                                                                                                                                                                                                                                        This movie with a...|\n",
      "|  5|A Cookbook Every ...|                                                                                                                                                                                                                                                                                                                                                                                        I found a copy of...|\n",
      "|  3|          good basic|                                                                                                                                                                                                                                                                                                                                                                                        \"The book is a ba...|\n",
      "|  3|nice screen for a...|                                                                                                                                                                                                                                                                                                                                                                                        I compared a few ...|\n",
      "|  3|Poor maps, no hos...|                                                                                                                                                                                                                                                                                                                                                                                        It's a good book,...|\n",
      "|  2|Profound then. Tr...|                                                                                                                                                                                                                                                                                                                                                                                        The narrative sty...|\n",
      "|  1|     A complete Bust|                                                                                                                                                                                                                                                                                                                                                                                        \"This game requir...|\n",
      "|  5|Barbie as Rapunze...|                                                                                                                                                                                                                                                                                                                                                                                        I purchased this ...|\n",
      "|  5|Best Game for You...|                                                                                                                                                                                                                                                                                                                                                                                        My daughter absol...|\n",
      "|  2|Not so good - ok ...|                                                                                                                                                                                                                                                                                                                                                                                        \"This game is pre...|\n",
      "|  1|NOT OS X but MAC ...|                                                                                                                                                                                                                                                                                                                                                                                        NEGATIVE: This ga...|\n",
      "|  4|Good Program, Lot...|                                                                                                                                                                                                                                                                                                                                                                                        My 4year old real...|\n",
      "|  3|A Blast From My Past|                                                                                                                                                                                                                                                                                                                                                                                        \"I once purchased...|\n",
      "|  1|  Very disappointed!|                                                                                                                                                                                                                                                                                                                                                                                        This perfume is j...|\n",
      "+---+--------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------------------------------------------------+----------------------------------+\n",
      "|  3|  more like funchuck|\"Gave this to my dad for a gag gift after directing \"\"Nunsense|\"\" he got a reall kick out of it!\"|\n",
      "+---+--------------------+--------------------------------------------------------------+----------------------------------+\n",
      "|  5|           Inspiring|                                          I hope a lot of p...|                              null|\n",
      "|  5|The best soundtra...|                                          I'm reading a lot...|                              null|\n",
      "|  4|    Chrono Cross OST|                                          \"The music of Yas...|               The Girl who Sto...|\n",
      "|  5| Too good to be true|                                          Probably the grea...|                              null|\n",
      "|  5|There's a reason ...|                                          There's a reason ...|                              null|\n",
      "|  1|        Buyer beware|                                          \"This is a self-p...|               unless you are i...|\n",
      "|  4|Errors, but great...|                                          I was a dissapoin...|                              null|\n",
      "|  1|          The Worst!|                                          A complete waste ...|                              null|\n",
      "|  1|           Oh please|                                          I guess you have ...|                              null|\n",
      "|  1|Awful beyond belief!|                                          \"I feel I have to...|               that I decided t...|\n",
      "|  4|A romantic zen ba...|                                          \"When you hear fo...|               three hysterical...|\n",
      "|  5|Lower leg comfort...|                                          Excellent stockin...|                              null|\n",
      "|  3|Delivery was very...|                                          It took almost 3 ...|                              null|\n",
      "|  2|sizes recomended ...|                                          sizes are much sm...|                              null|\n",
      "|  3|            Overbury|                                          Full of intrigue ...|                              null|\n",
      "|  1|Another Abysmal D...|                                          \"Rather than scra...|                              null|\n",
      "|  4|Wardell's book is...|                                          \"Steven Wardell's...|               they are bombard...|\n",
      "|  4|i liked this albu...|                                          \"I heard a song o...|              \"\"lanna\"\" and 'ma...|\n",
      "|  3|Better than I tho...|                                          I wrote a harsh r...|                              null|\n",
      "|  3|The great Roy Orb...|                                          When I saw the te...|                              null|\n",
      "+---+--------------------+--------------------------------------------------------------+----------------------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- 1: integer (nullable = true)\n",
      " |-- mens ultrasheer: string (nullable = true)\n",
      " |-- This model may be ok for sedentary types, but I'm active and get around alot in my job - consistently found these stockings rolled up down by my ankles! Not Good!! Solution: go with the standard compression stocking, 20-30, stock #114622. Excellent support, stays up and gives me what I need. Both pair of these also tore as I struggled to pull them up all the time. Good riddance/bad investment!: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- 3: integer (nullable = true)\n",
      " |-- more like funchuck: string (nullable = true)\n",
      " |-- \"Gave this to my dad for a gag gift after directing \"\"Nunsense: string (nullable = true)\n",
      " |-- \"\" he got a reall kick out of it!\": string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show first few rows\n",
    "test_data.show()\n",
    "train_data.show()\n",
    "\n",
    "# Show schema\n",
    "test_data.printSchema()\n",
    "train_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2c8cb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"review_text\", outputCol=\"tokens\")\n",
    "hashingTF = HashingTF(inputCol=\"tokens\", outputCol=\"rawFeatures\")\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f33a9d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "test_data = spark.read.csv(\n",
    "    \"hdfs://localhost:9000/user/hduser/CustomerReviews/test/test.csv\", \n",
    "    header=True, \n",
    "    inferSchema=True, \n",
    "    sep=\",\", \n",
    "    quote='\"'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "732dd29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Initialize classifier\n",
    "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\")\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, idf, lr])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "172c16a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 8:>                                                          (0 + 1) / 1]\r",
      "2023-09-19 21:58:48,626 WARN impl.BlockReaderFactory: I/O error constructing remote block reader.\n",
      "java.nio.channels.ClosedByInterruptException\n",
      "\tat java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)\n",
      "\tat sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:658)\n",
      "\tat org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)\n",
      "\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:586)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3025)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:826)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:751)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:381)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:755)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:685)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1647)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:851)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:893)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:957)\n",
      "\tat java.io.DataInputStream.read(DataInputStream.java:149)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)\n",
      "\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)\n",
      "\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)\n",
      "\tat org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:261)\n",
      "\tat org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:50)\n",
      "\tat org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:312)\n",
      "\tat org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:243)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:729)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:435)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2048)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:270)\n",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "text_data = sc.textFile(\"hdfs://localhost:9000/user/hduser/CustomerReviews/train/train.csv\")\n",
    "parsed_data = text_data.map(lambda line: Row(\n",
    "    column1=line.split(',')[0],\n",
    "    column2=line.split(',')[1],\n",
    "    # so on...\n",
    "))\n",
    "\n",
    "# Convert RDD to DataFrame\n",
    "train_data = spark.createDataFrame(parsed_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "32b373a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'iteritems'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31477/878390145.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"file:///home/hduser/Desktop/test.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    671\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhas_pandas\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m             \u001b[0;31m# Create a DataFrame from pandas DataFrame.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m             return super(SparkSession, self).createDataFrame(\n\u001b[0m\u001b[1;32m    674\u001b[0m                 data, schema, samplingRatio, verifySchema)\n\u001b[1;32m    675\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/pandas/conversion.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    337\u001b[0m                     \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_from_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimezone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/pandas/conversion.py\u001b[0m in \u001b[0;36m_convert_from_pandas\u001b[0;34m(self, pdf, schema, timezone)\u001b[0m\n\u001b[1;32m    369\u001b[0m                             \u001b[0mpdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfield\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseries\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m                     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_series_convert_timestamps_tz_local\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimezone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mseries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   6200\u001b[0m         ):\n\u001b[1;32m   6201\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6202\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6204\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'iteritems'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pdf = pd.read_csv(\"file:///home/hduser/Desktop/test.csv\")\n",
    "train_data = spark.createDataFrame(pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f97f21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
