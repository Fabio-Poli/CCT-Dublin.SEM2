{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c350b4a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyspark in /usr/local/spark-3.2.4-bin-hadoop3.2/python (3.2.4)\n",
      "Requirement already satisfied: py4j==0.10.9.5 in /home/hduser/.local/lib/python3.10/site-packages (from pyspark) (0.10.9.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59625b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51765d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "# Stop the existing SparkContext if any\n",
    "SparkContext.stop(sc)\n",
    "\n",
    "# Initialize a new SparkContext\n",
    "sc = SparkContext(master=\"local\", appName=\"New App\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21ad2ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Customer Reviews Analysis\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b26b6482",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "test_data = spark.read.csv(\"hdfs://localhost:9000/user/hduser/CustomerReviews/test/test.csv\", header=True, inferSchema=True)\n",
    "train_data = spark.read.csv(\"hdfs://localhost:9000/user/hduser/CustomerReviews/train/train.csv\", header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d859d760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|  1|     mens ultrasheer|This model may be ok for sedentary types, but I'm active and get around alot in my job - consistently found these stockings rolled up down by my ankles! Not Good!! Solution: go with the standard compression stocking, 20-30, stock #114622. Excellent support, stays up and gives me what I need. Both pair of these also tore as I struggled to pull them up all the time. Good riddance/bad investment!|\n",
      "+---+--------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|  4|Surprisingly deli...|                                                                                                                                                                                                                                                                                                                                                                                        This is a fast re...|\n",
      "|  2|Works, but not as...|                                                                                                                                                                                                                                                                                                                                                                                        \"I bought one of ...|\n",
      "|  2|             Oh dear|                                                                                                                                                                                                                                                                                                                                                                                        \"I was excited to...|\n",
      "|  2|     Incorrect disc!|                                                                                                                                                                                                                                                                                                                                                                                        \"I am a big JVC f...|\n",
      "|  2|      Incorrect Disc|                                                                                                                                                                                                                                                                                                                                                                                        I love the style ...|\n",
      "|  2|DVD menu select p...|                                                                                                                                                                                                                                                                                                                                                                                        I cannot scroll t...|\n",
      "|  3|My 2 y/o grandson...|                                                                                                                                                                                                                                                                                                                                                                                        This movie with a...|\n",
      "|  5|A Cookbook Every ...|                                                                                                                                                                                                                                                                                                                                                                                        I found a copy of...|\n",
      "|  3|          good basic|                                                                                                                                                                                                                                                                                                                                                                                        \"The book is a ba...|\n",
      "|  3|nice screen for a...|                                                                                                                                                                                                                                                                                                                                                                                        I compared a few ...|\n",
      "|  3|Poor maps, no hos...|                                                                                                                                                                                                                                                                                                                                                                                        It's a good book,...|\n",
      "|  2|Profound then. Tr...|                                                                                                                                                                                                                                                                                                                                                                                        The narrative sty...|\n",
      "|  1|     A complete Bust|                                                                                                                                                                                                                                                                                                                                                                                        \"This game requir...|\n",
      "|  5|Barbie as Rapunze...|                                                                                                                                                                                                                                                                                                                                                                                        I purchased this ...|\n",
      "|  5|Best Game for You...|                                                                                                                                                                                                                                                                                                                                                                                        My daughter absol...|\n",
      "|  2|Not so good - ok ...|                                                                                                                                                                                                                                                                                                                                                                                        \"This game is pre...|\n",
      "|  1|NOT OS X but MAC ...|                                                                                                                                                                                                                                                                                                                                                                                        NEGATIVE: This ga...|\n",
      "|  4|Good Program, Lot...|                                                                                                                                                                                                                                                                                                                                                                                        My 4year old real...|\n",
      "|  3|A Blast From My Past|                                                                                                                                                                                                                                                                                                                                                                                        \"I once purchased...|\n",
      "|  1|  Very disappointed!|                                                                                                                                                                                                                                                                                                                                                                                        This perfume is j...|\n",
      "+---+--------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+--------------------+--------------------------------------------------------------+----------------------------------+\n",
      "|  3|  more like funchuck|\"Gave this to my dad for a gag gift after directing \"\"Nunsense|\"\" he got a reall kick out of it!\"|\n",
      "+---+--------------------+--------------------------------------------------------------+----------------------------------+\n",
      "|  5|           Inspiring|                                          I hope a lot of p...|                              null|\n",
      "|  5|The best soundtra...|                                          I'm reading a lot...|                              null|\n",
      "|  4|    Chrono Cross OST|                                          \"The music of Yas...|               The Girl who Sto...|\n",
      "|  5| Too good to be true|                                          Probably the grea...|                              null|\n",
      "|  5|There's a reason ...|                                          There's a reason ...|                              null|\n",
      "|  1|        Buyer beware|                                          \"This is a self-p...|               unless you are i...|\n",
      "|  4|Errors, but great...|                                          I was a dissapoin...|                              null|\n",
      "|  1|          The Worst!|                                          A complete waste ...|                              null|\n",
      "|  1|           Oh please|                                          I guess you have ...|                              null|\n",
      "|  1|Awful beyond belief!|                                          \"I feel I have to...|               that I decided t...|\n",
      "|  4|A romantic zen ba...|                                          \"When you hear fo...|               three hysterical...|\n",
      "|  5|Lower leg comfort...|                                          Excellent stockin...|                              null|\n",
      "|  3|Delivery was very...|                                          It took almost 3 ...|                              null|\n",
      "|  2|sizes recomended ...|                                          sizes are much sm...|                              null|\n",
      "|  3|            Overbury|                                          Full of intrigue ...|                              null|\n",
      "|  1|Another Abysmal D...|                                          \"Rather than scra...|                              null|\n",
      "|  4|Wardell's book is...|                                          \"Steven Wardell's...|               they are bombard...|\n",
      "|  4|i liked this albu...|                                          \"I heard a song o...|              \"\"lanna\"\" and 'ma...|\n",
      "|  3|Better than I tho...|                                          I wrote a harsh r...|                              null|\n",
      "|  3|The great Roy Orb...|                                          When I saw the te...|                              null|\n",
      "+---+--------------------+--------------------------------------------------------------+----------------------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- 1: integer (nullable = true)\n",
      " |-- mens ultrasheer: string (nullable = true)\n",
      " |-- This model may be ok for sedentary types, but I'm active and get around alot in my job - consistently found these stockings rolled up down by my ankles! Not Good!! Solution: go with the standard compression stocking, 20-30, stock #114622. Excellent support, stays up and gives me what I need. Both pair of these also tore as I struggled to pull them up all the time. Good riddance/bad investment!: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- 3: integer (nullable = true)\n",
      " |-- more like funchuck: string (nullable = true)\n",
      " |-- \"Gave this to my dad for a gag gift after directing \"\"Nunsense: string (nullable = true)\n",
      " |-- \"\" he got a reall kick out of it!\": string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show first few rows\n",
    "test_data.show()\n",
    "train_data.show()\n",
    "\n",
    "# Show schema\n",
    "test_data.printSchema()\n",
    "train_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2c8cb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"review_text\", outputCol=\"tokens\")\n",
    "hashingTF = HashingTF(inputCol=\"tokens\", outputCol=\"rawFeatures\")\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f33a9d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "test_data = spark.read.csv(\n",
    "    \"hdfs://localhost:9000/user/hduser/CustomerReviews/test/test.csv\", \n",
    "    header=True, \n",
    "    inferSchema=True, \n",
    "    sep=\",\", \n",
    "    quote='\"'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "732dd29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Initialize classifier\n",
    "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\")\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, idf, lr])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84f97f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns in test_data and train_data\n",
    "test_data = test_data.withColumnRenamed(\"1\", \"rating\") \\\n",
    "                     .withColumnRenamed(\"mens ultrasheer\", \"title\") \\\n",
    "                     .withColumnRenamed(\"This model may be ok for sedentary types, but I'm active and get around alot in my job - consistently found these stockings rolled up down by my ankles! Not Good!! Solution: go with the standard compression stocking, 20-30, stock #114622. Excellent support, stays up and gives me what I need. Both pair of these also tore as I struggled to pull them up all the time. Good riddance/bad investment!\", \"review_text\")\n",
    "\n",
    "train_data = train_data.withColumnRenamed('3', 'rating') \\\n",
    "                       .withColumnRenamed('more like funchuck', 'title') \\\n",
    "                       .withColumnRenamed('\"Gave this to my dad for a gag gift after directing \"\"Nunsense', 'review_part1') \\\n",
    "                       .withColumnRenamed('\"\" he got a reall kick out of it!\"', 'review_part2')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f935acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rating', 'title', 'review_part1', 'review_part2']\n"
     ]
    }
   ],
   "source": [
    "print(train_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "465bdcf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rating', 'title', 'review_text']\n"
     ]
    }
   ],
   "source": [
    "print(test_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8d39de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat_ws\n",
    "\n",
    "train_data = train_data.withColumn(\"review_text\", concat_ws(\" \", train_data.review_part1, train_data.review_part2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3bf2752a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.withColumnRenamed(\"rating\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f1cfe4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: textblob in /home/hduser/.local/lib/python3.10/site-packages (0.17.1)\n",
      "Requirement already satisfied: nltk>=3.1 in /home/hduser/.local/lib/python3.10/site-packages (from textblob) (3.8.1)\n",
      "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from nltk>=3.1->textblob) (8.0.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/hduser/.local/lib/python3.10/site-packages (from nltk>=3.1->textblob) (2023.8.8)\n",
      "Requirement already satisfied: joblib in /home/hduser/.local/lib/python3.10/site-packages (from nltk>=3.1->textblob) (1.3.2)\n",
      "Requirement already satisfied: tqdm in /home/hduser/.local/lib/python3.10/site-packages (from nltk>=3.1->textblob) (4.66.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0b37730b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[label: int, title: string, review_part1: string, review_part2: string, review_text: string, sentiment_score: float]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import FloatType\n",
    "# If you can install TextBlob, uncomment the next line\n",
    "# from textblob import TextBlob\n",
    "\n",
    "# Define a UDF to compute sentiment score\n",
    "def sentiment_score(text):\n",
    "    return TextBlob(text).sentiment.polarity  # If you can't install TextBlob, replace this line with your sentiment analysis logic\n",
    "\n",
    "# Register the UDF\n",
    "sentiment_score_udf = udf(sentiment_score, FloatType())\n",
    "\n",
    "# Assuming your DataFrame is named 'train_data'\n",
    "# Add sentiment score as a new column to the DataFrame\n",
    "train_data_with_sentiment = train_data.withColumn(\"sentiment_score\", sentiment_score_udf(train_data['review_text']))\n",
    "\n",
    "# Show the DataFrame with the new column\n",
    "print(train_data_with_sentiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a2cbd896",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Define stages of the pipeline\n",
    "tokenizer = Tokenizer(inputCol=\"review_text\", outputCol=\"tokens\")\n",
    "hashingTF = HashingTF(inputCol=\"tokens\", outputCol=\"rawFeatures\")\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"tfidf_features\")\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"tfidf_features\", \"sentiment_score\"],\n",
    "    outputCol=\"final_features\"\n",
    ")\n",
    "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"final_features\")\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, idf, assembler, lr])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5fcb00fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: integer (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- review_part1: string (nullable = true)\n",
      " |-- review_part2: string (nullable = true)\n",
      " |-- review_text: string (nullable = false)\n",
      " |-- sentiment_score: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data_with_sentiment.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36bee255",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: textblob in /home/hduser/.local/lib/python3.10/site-packages (0.17.1)\n",
      "Requirement already satisfied: nltk>=3.1 in /home/hduser/.local/lib/python3.10/site-packages (from textblob) (3.8.1)\n",
      "Requirement already satisfied: tqdm in /home/hduser/.local/lib/python3.10/site-packages (from nltk>=3.1->textblob) (4.66.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/hduser/.local/lib/python3.10/site-packages (from nltk>=3.1->textblob) (2023.8.8)\n",
      "Requirement already satisfied: joblib in /home/hduser/.local/lib/python3.10/site-packages (from nltk>=3.1->textblob) (1.3.2)\n",
      "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from nltk>=3.1->textblob) (8.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3e57e47a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: textblob in /home/hduser/.local/lib/python3.10/site-packages (0.17.1)\n",
      "Requirement already satisfied: nltk>=3.1 in /home/hduser/.local/lib/python3.10/site-packages (from textblob) (3.8.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/hduser/.local/lib/python3.10/site-packages (from nltk>=3.1->textblob) (2023.8.8)\n",
      "Requirement already satisfied: joblib in /home/hduser/.local/lib/python3.10/site-packages (from nltk>=3.1->textblob) (1.3.2)\n",
      "Requirement already satisfied: tqdm in /home/hduser/.local/lib/python3.10/site-packages (from nltk>=3.1->textblob) (4.66.1)\n",
      "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from nltk>=3.1->textblob) (8.0.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "80efb492",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "039f4d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "def sentiment_score(text):\n",
    "    from textblob import TextBlob  # Importing inside the UDF\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "\n",
    "sentiment_score_udf = udf(sentiment_score, FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5ba40fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import site\n",
    "import textblob\n",
    "\n",
    "# Get the list of all site-packages directories\n",
    "site_packages = site.getsitepackages()\n",
    "\n",
    "# Find the path where textblob is installed\n",
    "textblob_path = textblob.__path__[0]\n",
    "\n",
    "# Loop over all site-packages to find where textblob is installed\n",
    "for site_package in site_packages:\n",
    "    if textblob_path.startswith(site_package):\n",
    "        print(f\"TextBlob is installed in: {site_package}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9fbc93a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "conf = SparkConf().setAppName(\"MyApp\").set('spark.executorEnv.PYTHONPATH', '/path/to/python/interpreter')\n",
    "sc = SparkContext.getOrCreate(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a8337fd2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: 15\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('basic_spark').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "data = [1, 2, 3, 4, 5]\n",
    "distData = sc.parallelize(data)\n",
    "\n",
    "result = distData.reduce(lambda a, b: a + b)\n",
    "print(\"Result:\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b98cd28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_PYTHON'] = '/usr/bin/python3.10' \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1da69771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3', '.', '1', '0', '.', '1', '2', ' ', '(', 'm', 'a', 'i', 'n', ',', ' ', 'J', 'u', 'n', ' ', '1', '1', ' ', '2', '0', '2', '3', ',', ' ', '0', '5', ':', '2', '6', ':', '2', '8', ')', ' ', '[', 'G', 'C', 'C', ' ', '1', '1', '.', '4', '.', '0', ']']\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def python_version(x):\n",
    "    import sys\n",
    "    return sys.version\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[1]\").appName('SparkApp').getOrCreate()\n",
    "print(spark.sparkContext.runJob(spark.sparkContext.parallelize([1]), python_version))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3e49c348",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 10:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+---------+\n",
      "| id|           text|sentiment|\n",
      "+---+---------------+---------+\n",
      "|  1|I love learning|      0.5|\n",
      "|  2|    I hate bugs|     -0.8|\n",
      "+---+---------------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Sentiment Analysis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.createDataFrame([(1, \"I love learning\"), (2, \"I hate bugs\")], [\"id\", \"text\"])\n",
    "\n",
    "df_with_sentiment = df.withColumn(\"sentiment\", sentiment_score_udf(df[\"text\"]))\n",
    "df_with_sentiment.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ffaf615e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_score(text):\n",
    "    try:\n",
    "        from textblob import TextBlob  # Importing inside the UDF\n",
    "        return TextBlob(text).sentiment.polarity\n",
    "    except Exception as e:\n",
    "        return str(e)  # Return the exception for debugging\n",
    "\n",
    "sentiment_score_udf = udf(sentiment_score, FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2b062941",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "def sentiment_score(text):\n",
    "    from textblob import TextBlob  # Explicit import\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "\n",
    "sentiment_score_udf = udf(sentiment_score, FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "15feda43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[label: int, title: string, review_part1: string, review_part2: string, review_text: string, sentiment_score: float]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import FloatType\n",
    "# If you can install TextBlob, uncomment the next line\n",
    "# from textblob import TextBlob\n",
    "\n",
    "# Define a UDF to compute sentiment score\n",
    "def sentiment_score(text):\n",
    "    return TextBlob(text).sentiment.polarity  # If you can't install TextBlob, replace this line with your sentiment analysis logic\n",
    "\n",
    "# Register the UDF\n",
    "sentiment_score_udf = udf(sentiment_score, FloatType())\n",
    "\n",
    "# Assuming your DataFrame is named 'train_data'\n",
    "# Add sentiment score as a new column to the DataFrame\n",
    "train_data_with_sentiment = train_data.withColumn(\"sentiment_score\", sentiment_score_udf(train_data['review_text']))\n",
    "\n",
    "# Show the DataFrame with the new column\n",
    "print(train_data_with_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0419a8a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 111] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_28007/3141492397.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mmaster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"local[1]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'SparkApp'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    231\u001b[0m                     \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m                     \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msessionState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetConfString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1318\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1320\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1034\u001b[0m          \u001b[0;32mif\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m         \"\"\"\n\u001b[0;32m-> 1036\u001b[0;31m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1037\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/clientserver.py\u001b[0m in \u001b[0;36m_get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconnection\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m             \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_new_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/clientserver.py\u001b[0m in \u001b[0;36m_create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_parameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython_parameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m             self.gateway_property, self)\n\u001b[0;32m--> 291\u001b[0;31m         \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect_to_java_server\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_thread_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/clientserver.py\u001b[0m in \u001b[0;36mconnect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    436\u001b[0m                 self.socket = self.ssl_context.wrap_socket(\n\u001b[1;32m    437\u001b[0m                     self.socket, server_hostname=self.java_address)\n\u001b[0;32m--> 438\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_address\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_port\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakefile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_connected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .config(\"spark.network.timeout\", \"10800s\") \\\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"3600s\") \\\n",
    "    .master(\"local[1]\") \\\n",
    "    .appName('SparkApp') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "18d7dab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Define stages of the pipeline\n",
    "tokenizer = Tokenizer(inputCol=\"review_text\", outputCol=\"tokens\")\n",
    "hashingTF = HashingTF(inputCol=\"tokens\", outputCol=\"rawFeatures\")\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"tfidf_features\")\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"tfidf_features\", \"sentiment_score\"],\n",
    "    outputCol=\"final_features\"\n",
    ")\n",
    "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"final_features\")\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, idf, assembler, lr])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6e546964",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-20 21:14:50,897 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 20.2 MiB\n",
      "2023-09-20 21:27:53,692 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 20.2 MiB\n",
      "2023-09-20 21:27:55,993 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 20.2 MiB\n",
      "2023-09-20 21:30:06,520 WARN memory.MemoryStore: Not enough space to cache rdd_119_1 in memory! (computed 113.1 MiB so far)\n",
      "2023-09-20 21:30:06,529 WARN storage.BlockManager: Persisting block rdd_119_1 to disk instead.\n",
      "2023-09-20 21:30:37,945 WARN memory.MemoryStore: Not enough space to cache rdd_119_1 in memory! (computed 113.1 MiB so far)\n",
      "2023-09-20 21:31:29,852 WARN memory.MemoryStore: Not enough space to cache rdd_119_2 in memory! (computed 113.1 MiB so far)\n",
      "2023-09-20 21:31:29,853 WARN storage.BlockManager: Persisting block rdd_119_2 to disk instead.\n",
      "2023-09-20 21:32:00,430 WARN memory.MemoryStore: Not enough space to cache rdd_119_2 in memory! (computed 113.1 MiB so far)\n",
      "2023-09-20 21:32:51,861 WARN memory.MemoryStore: Not enough space to cache rdd_119_3 in memory! (computed 113.1 MiB so far)\n",
      "2023-09-20 21:32:51,863 WARN storage.BlockManager: Persisting block rdd_119_3 to disk instead.\n",
      "2023-09-20 21:33:22,484 WARN memory.MemoryStore: Not enough space to cache rdd_119_3 in memory! (computed 113.1 MiB so far)\n",
      "2023-09-20 21:34:13,939 WARN memory.MemoryStore: Not enough space to cache rdd_119_4 in memory! (computed 113.1 MiB so far)\n",
      "2023-09-20 21:34:13,941 WARN storage.BlockManager: Persisting block rdd_119_4 to disk instead.\n",
      "2023-09-20 21:34:45,622 WARN memory.MemoryStore: Not enough space to cache rdd_119_4 in memory! (computed 113.1 MiB so far)\n",
      "2023-09-20 21:35:39,057 WARN memory.MemoryStore: Not enough space to cache rdd_119_5 in memory! (computed 113.0 MiB so far)\n",
      "2023-09-20 21:35:39,058 WARN storage.BlockManager: Persisting block rdd_119_5 to disk instead.\n",
      "2023-09-20 21:36:09,409 WARN memory.MemoryStore: Not enough space to cache rdd_119_5 in memory! (computed 113.0 MiB so far)\n",
      "2023-09-20 21:37:02,323 WARN memory.MemoryStore: Not enough space to cache rdd_119_6 in memory! (computed 113.1 MiB so far)\n",
      "2023-09-20 21:37:02,323 WARN storage.BlockManager: Persisting block rdd_119_6 to disk instead.\n",
      "2023-09-20 21:37:33,178 WARN memory.MemoryStore: Not enough space to cache rdd_119_6 in memory! (computed 113.1 MiB so far)\n",
      "2023-09-20 21:38:25,653 WARN memory.MemoryStore: Not enough space to cache rdd_119_7 in memory! (computed 113.1 MiB so far)\n",
      "2023-09-20 21:38:25,653 WARN storage.BlockManager: Persisting block rdd_119_7 to disk instead.\n",
      "2023-09-20 21:38:56,476 WARN memory.MemoryStore: Not enough space to cache rdd_119_7 in memory! (computed 113.1 MiB so far)\n",
      "2023-09-20 21:39:49,227 WARN memory.MemoryStore: Not enough space to cache rdd_119_8 in memory! (computed 113.1 MiB so far)\n",
      "2023-09-20 21:39:49,228 WARN storage.BlockManager: Persisting block rdd_119_8 to disk instead.\n",
      "2023-09-20 21:40:20,190 WARN memory.MemoryStore: Not enough space to cache rdd_119_8 in memory! (computed 113.1 MiB so far)\n",
      "2023-09-20 21:41:13,070 WARN memory.MemoryStore: Not enough space to cache rdd_119_9 in memory! (computed 113.1 MiB so far)\n",
      "2023-09-20 21:41:13,070 WARN storage.BlockManager: Persisting block rdd_119_9 to disk instead.\n",
      "2023-09-20 21:41:43,970 WARN memory.MemoryStore: Not enough space to cache rdd_119_9 in memory! (computed 113.1 MiB so far)\n",
      "2023-09-20 21:41:53,246 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 20.2 MiB\n",
      "2023-09-20 21:41:55,306 WARN netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "2023-09-20 21:41:55,317 WARN netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "2023-09-20 21:41:56,099 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 20.2 MiB\n",
      "2023-09-20 21:42:01,430 WARN memory.MemoryStore: Not enough space to cache rdd_119_1 in memory! (computed 113.1 MiB so far)\n",
      "2023-09-20 21:42:10,527 WARN memory.MemoryStore: Not enough space to cache rdd_119_2 in memory! (computed 113.1 MiB so far)\n",
      "2023-09-20 21:42:22,262 WARN memory.MemoryStore: Not enough space to cache rdd_119_3 in memory! (computed 113.1 MiB so far)\n",
      "2023-09-20 21:42:27,947 WARN memory.MemoryStore: Not enough space to cache rdd_119_4 in memory! (computed 113.1 MiB so far)\n",
      "2023-09-20 21:42:33,967 WARN memory.MemoryStore: Not enough space to cache rdd_119_5 in memory! (computed 113.0 MiB so far)\n",
      "2023-09-20 21:42:39,386 WARN memory.MemoryStore: Not enough space to cache rdd_119_6 in memory! (computed 113.1 MiB so far)\n",
      "2023-09-20 21:42:44,701 WARN memory.MemoryStore: Not enough space to cache rdd_119_7 in memory! (computed 113.1 MiB so far)\n",
      "2023-09-20 21:42:50,148 WARN memory.MemoryStore: Not enough space to cache rdd_119_8 in memory! (computed 113.1 MiB so far)\n",
      "2023-09-20 21:42:55,365 WARN memory.MemoryStore: Not enough space to cache rdd_119_9 in memory! (computed 113.1 MiB so far)\n",
      "2023-09-20 21:43:01,021 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 20.2 MiB\n",
      "2023-09-20 21:43:03,444 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 20.2 MiB\n",
      "2023-09-20 21:43:08,593 WARN memory.MemoryStore: Not enough space to cache rdd_119_1 in memory! (computed 113.1 MiB so far)\n",
      "2023-09-20 21:43:16,431 WARN memory.MemoryStore: Not enough space to cache rdd_119_2 in memory! (computed 113.1 MiB so far)\n",
      "2023-09-20 21:43:28,605 WARN memory.MemoryStore: Not enough space to cache rdd_119_3 in memory! (computed 113.1 MiB so far)\n",
      "2023-09-20 21:43:40,599 WARN memory.MemoryStore: Not enough space to cache rdd_119_4 in memory! (computed 113.1 MiB so far)\n",
      "2023-09-20 21:43:50,903 WARN memory.MemoryStore: Not enough space to cache rdd_119_5 in memory! (computed 113.0 MiB so far)\n",
      "2023-09-20 21:44:01,479 WARN memory.MemoryStore: Not enough space to cache rdd_119_6 in memory! (computed 113.1 MiB so far)\n",
      "2023-09-20 21:44:13,791 WARN memory.MemoryStore: Not enough space to cache rdd_119_7 in memory! (computed 113.1 MiB so far)\n",
      "2023-09-20 21:44:22,287 WARN memory.MemoryStore: Not enough space to cache rdd_119_8 in memory! (computed 113.1 MiB so far)\n",
      "2023-09-20 21:44:34,823 WARN memory.MemoryStore: Not enough space to cache rdd_119_9 in memory! (computed 113.1 MiB so far)\n",
      "2023-09-20 21:44:47,535 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 20.2 MiB\n",
      "2023-09-20 21:44:49,847 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 20.2 MiB\n",
      "2023-09-20 21:44:55,319 WARN memory.MemoryStore: Not enough space to cache rdd_119_1 in memory! (computed 113.1 MiB so far)\n",
      "2023-09-20 21:45:06,133 WARN memory.MemoryStore: Not enough space to cache rdd_119_2 in memory! (computed 113.1 MiB so far)\n",
      "2023-09-20 21:45:17,698 WARN memory.MemoryStore: Not enough space to cache rdd_119_3 in memory! (computed 113.1 MiB so far)\n",
      "2023-09-20 21:45:28,544 WARN memory.MemoryStore: Not enough space to cache rdd_119_4 in memory! (computed 113.1 MiB so far)\n",
      "2023-09-20 21:45:40,410 WARN memory.MemoryStore: Not enough space to cache rdd_119_5 in memory! (computed 113.0 MiB so far)\n",
      "2023-09-20 21:45:50,660 WARN memory.MemoryStore: Not enough space to cache rdd_119_6 in memory! (computed 113.1 MiB so far)\n",
      "2023-09-20 21:46:02,839 WARN memory.MemoryStore: Not enough space to cache rdd_119_7 in memory! (computed 113.1 MiB so far)\n",
      "2023-09-20 21:46:11,856 WARN memory.MemoryStore: Not enough space to cache rdd_119_8 in memory! (computed 113.1 MiB so far)\n",
      "2023-09-20 21:46:22,480 WARN memory.MemoryStore: Not enough space to cache rdd_119_9 in memory! (computed 113.1 MiB so far)\n",
      "2023-09-20 21:46:33,388 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 20.2 MiB\n",
      "2023-09-20 21:46:35,442 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 20.2 MiB\n",
      "2023-09-20 21:46:40,653 WARN memory.MemoryStore: Not enough space to cache rdd_119_1 in memory! (computed 113.1 MiB so far)\n",
      "2023-09-20 21:46:51,717 WARN memory.MemoryStore: Not enough space to cache rdd_119_2 in memory! (computed 113.1 MiB so far)\n",
      "2023-09-20 21:47:10,770 WARN memory.MemoryStore: Not enough space to cache rdd_119_3 in memory! (computed 113.1 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-20 21:47:27,951 WARN memory.MemoryStore: Not enough space to cache rdd_119_4 in memory! (computed 113.1 MiB so far)\n",
      "2023-09-20 21:47:48,193 WARN memory.MemoryStore: Not enough space to cache rdd_119_5 in memory! (computed 113.0 MiB so far)\n",
      "2023-09-20 21:48:04,975 WARN memory.MemoryStore: Not enough space to cache rdd_119_6 in memory! (computed 113.1 MiB so far)\n",
      "2023-09-20 21:48:26,540 WARN memory.MemoryStore: Not enough space to cache rdd_119_7 in memory! (computed 113.1 MiB so far)\n",
      "2023-09-20 21:48:47,370 WARN memory.MemoryStore: Not enough space to cache rdd_119_8 in memory! (computed 113.1 MiB so far)\n",
      "2023-09-20 21:49:08,534 WARN memory.MemoryStore: Not enough space to cache rdd_119_9 in memory! (computed 113.1 MiB so far)\n",
      "2023-09-20 21:49:30,408 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 20.2 MiB\n",
      "2023-09-20 21:49:33,315 WARN storage.BlockManager: Asked to remove block broadcast_47_piece3, which does not exist\n",
      "2023-09-20 21:49:33,315 WARN storage.BlockManager: Asked to remove block broadcast_47_piece1, which does not exist\n",
      "2023-09-20 21:49:34,421 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 20.2 MiB\n",
      "2023-09-20 21:49:51,520 WARN memory.MemoryStore: Not enough space to cache rdd_119_1 in memory! (computed 113.1 MiB so far)\n",
      "2023-09-20 21:50:20,068 WARN memory.MemoryStore: Not enough space to cache rdd_119_2 in memory! (computed 113.1 MiB so far)\n",
      "2023-09-20 21:50:48,327 WARN memory.MemoryStore: Not enough space to cache rdd_119_3 in memory! (computed 113.1 MiB so far)\n",
      "2023-09-20 21:51:17,272 WARN memory.MemoryStore: Not enough space to cache rdd_119_4 in memory! (computed 113.1 MiB so far)\n",
      "2023-09-20 21:51:46,200 WARN memory.MemoryStore: Not enough space to cache rdd_119_5 in memory! (computed 113.0 MiB so far)\n",
      "2023-09-20 21:52:14,523 WARN memory.MemoryStore: Not enough space to cache rdd_119_6 in memory! (computed 113.1 MiB so far)\n",
      "2023-09-20 21:52:41,958 WARN memory.MemoryStore: Not enough space to cache rdd_119_7 in memory! (computed 113.1 MiB so far)\n",
      "2023-09-20 21:53:10,467 WARN memory.MemoryStore: Not enough space to cache rdd_119_8 in memory! (computed 113.1 MiB so far)\n",
      "2023-09-20 21:53:38,978 WARN memory.MemoryStore: Not enough space to cache rdd_119_9 in memory! (computed 113.1 MiB so far)\n",
      "2023-09-20 21:54:09,074 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 20.2 MiB\n",
      "2023-09-20 21:54:12,429 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 20.2 MiB\n",
      "2023-09-20 21:54:29,111 WARN memory.MemoryStore: Not enough space to cache rdd_119_1 in memory! (computed 113.1 MiB so far)\n",
      "2023-09-20 21:55:22,350 WARN memory.MemoryStore: Not enough space to cache rdd_119_2 in memory! (computed 113.1 MiB so far)\n",
      "2023-09-20 21:56:15,673 WARN memory.MemoryStore: Not enough space to cache rdd_119_3 in memory! (computed 113.1 MiB so far)\n",
      "2023-09-20 21:57:09,201 WARN memory.MemoryStore: Not enough space to cache rdd_119_4 in memory! (computed 113.1 MiB so far)\n",
      "2023-09-20 21:58:02,881 WARN memory.MemoryStore: Not enough space to cache rdd_119_5 in memory! (computed 113.0 MiB so far)\n",
      "2023-09-20 21:58:56,852 WARN memory.MemoryStore: Not enough space to cache rdd_119_6 in memory! (computed 113.1 MiB so far)\n",
      "2023-09-20 21:59:52,942 WARN memory.MemoryStore: Not enough space to cache rdd_119_7 in memory! (computed 113.1 MiB so far)\n",
      "2023-09-20 22:00:47,363 WARN memory.MemoryStore: Not enough space to cache rdd_119_8 in memory! (computed 113.1 MiB so far)\n",
      "2023-09-20 22:01:42,506 WARN memory.MemoryStore: Not enough space to cache rdd_119_9 in memory! (computed 113.1 MiB so far)\n",
      "2023-09-20 22:02:37,746 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 20.2 MiB\n",
      "2023-09-20 22:02:40,784 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 20.2 MiB\n",
      "2023-09-20 22:02:58,467 WARN memory.MemoryStore: Not enough space to cache rdd_119_1 in memory! (computed 113.1 MiB so far)\n",
      "2023-09-20 22:02:59,122 ERROR executor.Executor: Exception in task 1.0 in stage 34.0 (TID 159)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat scala.reflect.ManifestFactory$DoubleManifest.newArray(Manifest.scala:194)\n",
      "\tat scala.reflect.ManifestFactory$DoubleManifest.newArray(Manifest.scala:191)\n",
      "\tat scala.Array$.ofDim(Array.scala:305)\n",
      "\tat org.apache.spark.ml.optim.aggregator.DifferentiableLossAggregator.gradientSumArray(DifferentiableLossAggregator.scala:43)\n",
      "\tat org.apache.spark.ml.optim.aggregator.DifferentiableLossAggregator.gradientSumArray$(DifferentiableLossAggregator.scala:43)\n",
      "\tat org.apache.spark.ml.optim.aggregator.MultinomialLogisticBlockAggregator.gradientSumArray$lzycompute(MultinomialLogisticBlockAggregator.scala:45)\n",
      "\tat org.apache.spark.ml.optim.aggregator.MultinomialLogisticBlockAggregator.gradientSumArray(MultinomialLogisticBlockAggregator.scala:45)\n",
      "\tat org.apache.spark.ml.optim.aggregator.MultinomialLogisticBlockAggregator.$anonfun$add$5(MultinomialLogisticBlockAggregator.scala:162)\n",
      "\tat org.apache.spark.ml.optim.aggregator.MultinomialLogisticBlockAggregator.$anonfun$add$5$adapted(MultinomialLogisticBlockAggregator.scala:162)\n",
      "\tat org.apache.spark.ml.optim.aggregator.MultinomialLogisticBlockAggregator$$Lambda$3681/346793338.apply(Unknown Source)\n",
      "\tat org.apache.spark.ml.linalg.DenseMatrix.foreachActive(Matrices.scala:385)\n",
      "\tat org.apache.spark.ml.optim.aggregator.MultinomialLogisticBlockAggregator.add(MultinomialLogisticBlockAggregator.scala:162)\n",
      "\tat org.apache.spark.ml.optim.aggregator.MultinomialLogisticBlockAggregator.add(MultinomialLogisticBlockAggregator.scala:45)\n",
      "\tat org.apache.spark.ml.optim.loss.RDDLossFunction.$anonfun$calculate$1(RDDLossFunction.scala:59)\n",
      "\tat org.apache.spark.ml.optim.loss.RDDLossFunction$$Lambda$3641/401773725.apply(Unknown Source)\n",
      "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:196)\n",
      "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:194)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat org.apache.spark.InterruptibleIterator.foldLeft(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n",
      "\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n",
      "\tat org.apache.spark.InterruptibleIterator.aggregate(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$3(RDD.scala:1230)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$3228/69611501.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$5(RDD.scala:1231)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$3229/154231387.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)\n",
      "2023-09-20 22:02:59,142 ERROR util.SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 1.0 in stage 34.0 (TID 159),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat scala.reflect.ManifestFactory$DoubleManifest.newArray(Manifest.scala:194)\n",
      "\tat scala.reflect.ManifestFactory$DoubleManifest.newArray(Manifest.scala:191)\n",
      "\tat scala.Array$.ofDim(Array.scala:305)\n",
      "\tat org.apache.spark.ml.optim.aggregator.DifferentiableLossAggregator.gradientSumArray(DifferentiableLossAggregator.scala:43)\n",
      "\tat org.apache.spark.ml.optim.aggregator.DifferentiableLossAggregator.gradientSumArray$(DifferentiableLossAggregator.scala:43)\n",
      "\tat org.apache.spark.ml.optim.aggregator.MultinomialLogisticBlockAggregator.gradientSumArray$lzycompute(MultinomialLogisticBlockAggregator.scala:45)\n",
      "\tat org.apache.spark.ml.optim.aggregator.MultinomialLogisticBlockAggregator.gradientSumArray(MultinomialLogisticBlockAggregator.scala:45)\n",
      "\tat org.apache.spark.ml.optim.aggregator.MultinomialLogisticBlockAggregator.$anonfun$add$5(MultinomialLogisticBlockAggregator.scala:162)\n",
      "\tat org.apache.spark.ml.optim.aggregator.MultinomialLogisticBlockAggregator.$anonfun$add$5$adapted(MultinomialLogisticBlockAggregator.scala:162)\n",
      "\tat org.apache.spark.ml.optim.aggregator.MultinomialLogisticBlockAggregator$$Lambda$3681/346793338.apply(Unknown Source)\n",
      "\tat org.apache.spark.ml.linalg.DenseMatrix.foreachActive(Matrices.scala:385)\n",
      "\tat org.apache.spark.ml.optim.aggregator.MultinomialLogisticBlockAggregator.add(MultinomialLogisticBlockAggregator.scala:162)\n",
      "\tat org.apache.spark.ml.optim.aggregator.MultinomialLogisticBlockAggregator.add(MultinomialLogisticBlockAggregator.scala:45)\n",
      "\tat org.apache.spark.ml.optim.loss.RDDLossFunction.$anonfun$calculate$1(RDDLossFunction.scala:59)\n",
      "\tat org.apache.spark.ml.optim.loss.RDDLossFunction$$Lambda$3641/401773725.apply(Unknown Source)\n",
      "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:196)\n",
      "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:194)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat org.apache.spark.InterruptibleIterator.foldLeft(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n",
      "\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n",
      "\tat org.apache.spark.InterruptibleIterator.aggregate(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$3(RDD.scala:1230)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$3228/69611501.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$5(RDD.scala:1231)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$3229/154231387.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)\n",
      "2023-09-20 22:02:59,147 WARN scheduler.TaskSetManager: Lost task 1.0 in stage 34.0 (TID 159) (fabio-poli-vm executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat scala.reflect.ManifestFactory$DoubleManifest.newArray(Manifest.scala:194)\n",
      "\tat scala.reflect.ManifestFactory$DoubleManifest.newArray(Manifest.scala:191)\n",
      "\tat scala.Array$.ofDim(Array.scala:305)\n",
      "\tat org.apache.spark.ml.optim.aggregator.DifferentiableLossAggregator.gradientSumArray(DifferentiableLossAggregator.scala:43)\n",
      "\tat org.apache.spark.ml.optim.aggregator.DifferentiableLossAggregator.gradientSumArray$(DifferentiableLossAggregator.scala:43)\n",
      "\tat org.apache.spark.ml.optim.aggregator.MultinomialLogisticBlockAggregator.gradientSumArray$lzycompute(MultinomialLogisticBlockAggregator.scala:45)\n",
      "\tat org.apache.spark.ml.optim.aggregator.MultinomialLogisticBlockAggregator.gradientSumArray(MultinomialLogisticBlockAggregator.scala:45)\n",
      "\tat org.apache.spark.ml.optim.aggregator.MultinomialLogisticBlockAggregator.$anonfun$add$5(MultinomialLogisticBlockAggregator.scala:162)\n",
      "\tat org.apache.spark.ml.optim.aggregator.MultinomialLogisticBlockAggregator.$anonfun$add$5$adapted(MultinomialLogisticBlockAggregator.scala:162)\n",
      "\tat org.apache.spark.ml.optim.aggregator.MultinomialLogisticBlockAggregator$$Lambda$3681/346793338.apply(Unknown Source)\n",
      "\tat org.apache.spark.ml.linalg.DenseMatrix.foreachActive(Matrices.scala:385)\n",
      "\tat org.apache.spark.ml.optim.aggregator.MultinomialLogisticBlockAggregator.add(MultinomialLogisticBlockAggregator.scala:162)\n",
      "\tat org.apache.spark.ml.optim.aggregator.MultinomialLogisticBlockAggregator.add(MultinomialLogisticBlockAggregator.scala:45)\n",
      "\tat org.apache.spark.ml.optim.loss.RDDLossFunction.$anonfun$calculate$1(RDDLossFunction.scala:59)\n",
      "\tat org.apache.spark.ml.optim.loss.RDDLossFunction$$Lambda$3641/401773725.apply(Unknown Source)\n",
      "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:196)\n",
      "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:194)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat org.apache.spark.InterruptibleIterator.foldLeft(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n",
      "\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n",
      "\tat org.apache.spark.InterruptibleIterator.aggregate(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$3(RDD.scala:1230)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$3228/69611501.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$5(RDD.scala:1231)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$3229/154231387.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)\n",
      "\n",
      "2023-09-20 22:02:59,147 ERROR scheduler.TaskSetManager: Task 1 in stage 34.0 failed 1 times; aborting job\n",
      "2023-09-20 22:02:59,313 ERROR util.Instrumentation: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 34.0 failed 1 times, most recent failure: Lost task 1.0 in stage 34.0 (TID 159) (fabio-poli-vm executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat scala.reflect.ManifestFactory$DoubleManifest.newArray(Manifest.scala:194)\n",
      "\tat scala.reflect.ManifestFactory$DoubleManifest.newArray(Manifest.scala:191)\n",
      "\tat scala.Array$.ofDim(Array.scala:305)\n",
      "\tat org.apache.spark.ml.optim.aggregator.DifferentiableLossAggregator.gradientSumArray(DifferentiableLossAggregator.scala:43)\n",
      "\tat org.apache.spark.ml.optim.aggregator.DifferentiableLossAggregator.gradientSumArray$(DifferentiableLossAggregator.scala:43)\n",
      "\tat org.apache.spark.ml.optim.aggregator.MultinomialLogisticBlockAggregator.gradientSumArray$lzycompute(MultinomialLogisticBlockAggregator.scala:45)\n",
      "\tat org.apache.spark.ml.optim.aggregator.MultinomialLogisticBlockAggregator.gradientSumArray(MultinomialLogisticBlockAggregator.scala:45)\n",
      "\tat org.apache.spark.ml.optim.aggregator.MultinomialLogisticBlockAggregator.$anonfun$add$5(MultinomialLogisticBlockAggregator.scala:162)\n",
      "\tat org.apache.spark.ml.optim.aggregator.MultinomialLogisticBlockAggregator.$anonfun$add$5$adapted(MultinomialLogisticBlockAggregator.scala:162)\n",
      "\tat org.apache.spark.ml.optim.aggregator.MultinomialLogisticBlockAggregator$$Lambda$3681/346793338.apply(Unknown Source)\n",
      "\tat org.apache.spark.ml.linalg.DenseMatrix.foreachActive(Matrices.scala:385)\n",
      "\tat org.apache.spark.ml.optim.aggregator.MultinomialLogisticBlockAggregator.add(MultinomialLogisticBlockAggregator.scala:162)\n",
      "\tat org.apache.spark.ml.optim.aggregator.MultinomialLogisticBlockAggregator.add(MultinomialLogisticBlockAggregator.scala:45)\n",
      "\tat org.apache.spark.ml.optim.loss.RDDLossFunction.$anonfun$calculate$1(RDDLossFunction.scala:59)\n",
      "\tat org.apache.spark.ml.optim.loss.RDDLossFunction$$Lambda$3641/401773725.apply(Unknown Source)\n",
      "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:196)\n",
      "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:194)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat org.apache.spark.InterruptibleIterator.foldLeft(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n",
      "\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n",
      "\tat org.apache.spark.InterruptibleIterator.aggregate(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$3(RDD.scala:1230)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$3228/69611501.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$5(RDD.scala:1231)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$3229/154231387.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2450)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2399)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2398)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2398)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1156)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1156)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1156)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2638)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2580)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2569)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2224)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2319)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$fold$1(RDD.scala:1183)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n",
      "\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1177)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$1(RDD.scala:1246)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n",
      "\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1222)\n",
      "\tat org.apache.spark.ml.optim.loss.RDDLossFunction.calculate(RDDLossFunction.scala:61)\n",
      "\tat org.apache.spark.ml.optim.loss.RDDLossFunction.calculate(RDDLossFunction.scala:47)\n",
      "\tat breeze.optimize.CachedDiffFunction.calculate(CachedDiffFunction.scala:24)\n",
      "\tat breeze.optimize.LineSearch$$anon$1.calculate(LineSearch.scala:52)\n",
      "\tat breeze.optimize.LineSearch$$anon$1.calculate(LineSearch.scala:31)\n",
      "\tat breeze.optimize.StrongWolfeLineSearch.phi$1(StrongWolfe.scala:76)\n",
      "\tat breeze.optimize.StrongWolfeLineSearch.$anonfun$minimizeWithBound$7(StrongWolfe.scala:152)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:158)\n",
      "\tat breeze.optimize.StrongWolfeLineSearch.minimizeWithBound(StrongWolfe.scala:151)\n",
      "\tat breeze.optimize.StrongWolfeLineSearch.minimize(StrongWolfe.scala:62)\n",
      "\tat breeze.optimize.LBFGS.determineStepSize(LBFGS.scala:82)\n",
      "\tat breeze.optimize.LBFGS.determineStepSize(LBFGS.scala:38)\n",
      "\tat breeze.optimize.FirstOrderMinimizer.$anonfun$infiniteIterations$1(FirstOrderMinimizer.scala:60)\n",
      "\tat scala.collection.Iterator$$anon$7.next(Iterator.scala:140)\n",
      "\tat breeze.util.IteratorImplicits$RichIterator$$anon$2.next(Implicits.scala:74)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression.trainImpl(LogisticRegression.scala:1009)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression.$anonfun$train$1(LogisticRegression.scala:628)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:495)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:286)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:151)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:115)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
      "\tat scala.reflect.ManifestFactory$DoubleManifest.newArray(Manifest.scala:194)\n",
      "\tat scala.reflect.ManifestFactory$DoubleManifest.newArray(Manifest.scala:191)\n",
      "\tat scala.Array$.ofDim(Array.scala:305)\n",
      "\tat org.apache.spark.ml.optim.aggregator.DifferentiableLossAggregator.gradientSumArray(DifferentiableLossAggregator.scala:43)\n",
      "\tat org.apache.spark.ml.optim.aggregator.DifferentiableLossAggregator.gradientSumArray$(DifferentiableLossAggregator.scala:43)\n",
      "\tat org.apache.spark.ml.optim.aggregator.MultinomialLogisticBlockAggregator.gradientSumArray$lzycompute(MultinomialLogisticBlockAggregator.scala:45)\n",
      "\tat org.apache.spark.ml.optim.aggregator.MultinomialLogisticBlockAggregator.gradientSumArray(MultinomialLogisticBlockAggregator.scala:45)\n",
      "\tat org.apache.spark.ml.optim.aggregator.MultinomialLogisticBlockAggregator.$anonfun$add$5(MultinomialLogisticBlockAggregator.scala:162)\n",
      "\tat org.apache.spark.ml.optim.aggregator.MultinomialLogisticBlockAggregator.$anonfun$add$5$adapted(MultinomialLogisticBlockAggregator.scala:162)\n",
      "\tat org.apache.spark.ml.optim.aggregator.MultinomialLogisticBlockAggregator$$Lambda$3681/346793338.apply(Unknown Source)\n",
      "\tat org.apache.spark.ml.linalg.DenseMatrix.foreachActive(Matrices.scala:385)\n",
      "\tat org.apache.spark.ml.optim.aggregator.MultinomialLogisticBlockAggregator.add(MultinomialLogisticBlockAggregator.scala:162)\n",
      "\tat org.apache.spark.ml.optim.aggregator.MultinomialLogisticBlockAggregator.add(MultinomialLogisticBlockAggregator.scala:45)\n",
      "\tat org.apache.spark.ml.optim.loss.RDDLossFunction.$anonfun$calculate$1(RDDLossFunction.scala:59)\n",
      "\tat org.apache.spark.ml.optim.loss.RDDLossFunction$$Lambda$3641/401773725.apply(Unknown Source)\n",
      "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:196)\n",
      "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:194)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat org.apache.spark.InterruptibleIterator.foldLeft(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n",
      "\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n",
      "\tat org.apache.spark.InterruptibleIterator.aggregate(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$3(RDD.scala:1230)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$3228/69611501.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$5(RDD.scala:1231)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$3229/154231387.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)\n",
      "\n",
      "2023-09-20 22:03:00,924 ERROR util.Utils: Uncaught exception in thread Executor task launch worker for task 2.0 in stage 34.0 (TID 160)\n",
      "java.lang.NullPointerException\n",
      "\tat org.apache.spark.scheduler.Task.$anonfun$run$2(Task.scala:152)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1471)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:150)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 45488)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib/python3.10/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib/python3.10/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib/python3.10/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 262, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 239, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/usr/local/spark/python/pyspark/serializers.py\", line 564, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py\", line 3457, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_28007/3564217736.py\", line 1, in <module>\n",
      "    model = pipeline.fit(train_data_with_sentiment)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/base.py\", line 161, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/pipeline.py\", line 114, in _fit\n",
      "    model = stage.fit(dataset)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/base.py\", line 161, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/wrapper.py\", line 339, in _fit\n",
      "    java_model = self._fit_java(dataset)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/wrapper.py\", line 336, in _fit_java\n",
      "    return self._java_obj.fit(dataset._jdf)\n",
      "  File \"/home/hduser/.local/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/local/spark/python/pyspark/sql/utils.py\", line 111, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/home/hduser/.local/lib/python3.10/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: <unprintable Py4JJavaError object>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py\", line 2077, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JJavaError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hduser/.local/lib/python3.10/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hduser/.local/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/hduser/.local/lib/python3.10/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 111] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_28007/3564217736.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_with_sentiment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    113\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# must be an Estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m                     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m                     \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31m<class 'str'>\u001b[0m: (<class 'ConnectionRefusedError'>, ConnectionRefusedError(111, 'Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2080\u001b[0m                                             value, tb, tb_offset=tb_offset)\n\u001b[1;32m   2081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2082\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_showtraceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2083\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_pdb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2084\u001b[0m                         \u001b[0;31m# drop into debugger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/ipykernel/zmqshell.py\u001b[0m in \u001b[0;36m_showtraceback\u001b[0;34m(self, etype, evalue, stb)\u001b[0m\n\u001b[1;32m    540\u001b[0m             \u001b[0;34m'traceback'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mstb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m             \u001b[0;34m'ename'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0;34m'evalue'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m         }\n\u001b[1;32m    544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36m__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    469\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__str__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m         \u001b[0mgateway_client\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway_client\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception_cmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m         \u001b[0mreturn_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_return_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m         \u001b[0;31m# Note: technically this should return a bytestring 'str' rather than\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1034\u001b[0m          \u001b[0;32mif\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m         \"\"\"\n\u001b[0;32m-> 1036\u001b[0;31m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1037\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/clientserver.py\u001b[0m in \u001b[0;36m_get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconnection\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m             \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_new_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/clientserver.py\u001b[0m in \u001b[0;36m_create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_parameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython_parameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m             self.gateway_property, self)\n\u001b[0;32m--> 291\u001b[0;31m         \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect_to_java_server\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_thread_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/clientserver.py\u001b[0m in \u001b[0;36mconnect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    436\u001b[0m                 self.socket = self.ssl_context.wrap_socket(\n\u001b[1;32m    437\u001b[0m                     self.socket, server_hostname=self.java_address)\n\u001b[0;32m--> 438\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_address\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_port\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakefile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_connected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused"
     ]
    }
   ],
   "source": [
    "model = pipeline.fit(train_data_with_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1409746",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
