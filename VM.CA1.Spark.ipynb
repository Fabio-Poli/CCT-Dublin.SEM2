{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e633ecb0",
   "metadata": {},
   "source": [
    "# 1. Install Packages and Import Libraries #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e317192c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pip in /home/hduser/.local/lib/python3.10/site-packages (23.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbdc98bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyspark in /usr/local/spark-3.2.4-bin-hadoop3.2/python (3.2.4)\n",
      "Requirement already satisfied: textblob in /home/hduser/.local/lib/python3.10/site-packages (0.17.1)\n",
      "Requirement already satisfied: py4j==0.10.9.5 in /home/hduser/.local/lib/python3.10/site-packages (from pyspark) (0.10.9.5)\n",
      "Requirement already satisfied: nltk>=3.1 in /home/hduser/.local/lib/python3.10/site-packages (from textblob) (3.8.1)\n",
      "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from nltk>=3.1->textblob) (8.0.3)\n",
      "Requirement already satisfied: joblib in /home/hduser/.local/lib/python3.10/site-packages (from nltk>=3.1->textblob) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/hduser/.local/lib/python3.10/site-packages (from nltk>=3.1->textblob) (2023.8.8)\n",
      "Requirement already satisfied: tqdm in /home/hduser/.local/lib/python3.10/site-packages (from nltk>=3.1->textblob) (4.66.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark textblob\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import udf, concat_ws\n",
    "from pyspark.sql.types import FloatType\n",
    "from textblob import TextBlob\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fd89890",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName('Your App') \\\n",
    "    .config('spark.default.parallelism', 100) \\\n",
    "    .config('spark.sql.shuffle.partitions', 100) \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea3fb94",
   "metadata": {},
   "source": [
    "# 2. Initialize Spark Session and Context #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc606633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"YourAppName\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Initialize Spark Context\n",
    "sc = SparkContext.getOrCreate(SparkConf().setAppName(\"New App\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d4a7ab",
   "metadata": {},
   "source": [
    "# 3. Read Data #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a45cf57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Read data from HDFS\n",
    "test_data = spark.read.csv(\"hdfs://localhost:9000/user/hduser/CustomerReviews/test/test.csv\", header=True, inferSchema=True)\n",
    "train_data = spark.read.csv(\"hdfs://localhost:9000/user/hduser/CustomerReviews/train/train.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd31a60",
   "metadata": {},
   "source": [
    "# 4. Preprocess Data #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f9ff67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns in test_data and train_data\n",
    "test_data = test_data.withColumnRenamed(\"1\", \"rating\") \\\n",
    "                     .withColumnRenamed(\"mens ultrasheer\", \"title\") \\\n",
    "                     .withColumnRenamed(\"This model may be ok for sedentary types, but I'm active and get around alot in my job - consistently found these stockings rolled up down by my ankles! Not Good!! Solution: go with the standard compression stocking, 20-30, stock #114622. Excellent support, stays up and gives me what I need. Both pair of these also tore as I struggled to pull them up all the time. Good riddance/bad investment!\", \"review_text\")\n",
    "\n",
    "train_data = train_data.withColumnRenamed('3', 'rating') \\\n",
    "                       .withColumnRenamed('more like funchuck', 'title') \\\n",
    "                       .withColumnRenamed('\"Gave this to my dad for a gag gift after directing \"\"Nunsense', 'review_part1') \\\n",
    "                       .withColumnRenamed('\"\" he got a reall kick out of it!\"', 'review_part2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6adfe828",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.withColumn(\"review_text\", concat_ws(\" \", train_data.review_part1, train_data.review_part2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff3a882",
   "metadata": {},
   "source": [
    "# 5. Text Sentiment Analysis UDF #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba318af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a UDF to compute sentiment score\n",
    "def sentiment_score(text):\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "\n",
    "sentiment_score_udf = udf(sentiment_score, FloatType())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcfd43f",
   "metadata": {},
   "source": [
    "# 6. Add Sentiment Score to DataFrame #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e7dcfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add sentiment score as a new column to train_data\n",
    "train_data_with_sentiment = train_data.withColumn(\"sentiment_score\", sentiment_score_udf(train_data['review_text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b61bf2",
   "metadata": {},
   "source": [
    "# 7. Build ML Pipeline #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1898336f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stages of the pipeline\n",
    "tokenizer = Tokenizer(inputCol=\"review_text\", outputCol=\"tokens\")\n",
    "hashingTF = HashingTF(inputCol=\"tokens\", outputCol=\"rawFeatures\")\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"tfidf_features\")\n",
    "assembler = VectorAssembler(inputCols=[\"tfidf_features\", \"sentiment_score\"], outputCol=\"final_features\")\n",
    "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"final_features\")\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, idf, assembler, lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51caae31",
   "metadata": {},
   "source": [
    "### Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c88ff90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- rating: integer (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- review_part1: string (nullable = true)\n",
      " |-- review_part2: string (nullable = true)\n",
      " |-- review_text: string (nullable = false)\n",
      " |-- sentiment_score: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data_with_sentiment.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ab1bdb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+---------+\n",
      "| id|           text|sentiment|\n",
      "+---+---------------+---------+\n",
      "|  1|I love learning|      0.5|\n",
      "|  2|    I hate bugs|     -0.8|\n",
      "+---+---------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Sentiment Analysis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.createDataFrame([(1, \"I love learning\"), (2, \"I hate bugs\")], [\"id\", \"text\"])\n",
    "\n",
    "df_with_sentiment = df.withColumn(\"sentiment\", sentiment_score_udf(df[\"text\"]))\n",
    "df_with_sentiment.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1bd6ff39",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.withColumnRenamed('rating', 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a72a99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 10% of the data for training\n",
    "sample_fraction = 0.0001\n",
    "sampled_train_data = train_data.sample(False, sample_fraction)\n",
    "\n",
    "# Add sentiment score to the sampled data\n",
    "sampled_train_data_with_sentiment = sampled_train_data.withColumn(\"sentiment_score\", sentiment_score_udf(sampled_train_data['review_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "558993be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records in original DataFrame:  2999999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:====================================>                    (7 + 4) / 11]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records in sampled DataFrame:  329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 10:==================================================>     (10 + 1) / 11]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Show the number of records for sanity check\n",
    "print(\"Total records in original DataFrame: \", train_data.count())\n",
    "print(\"Total records in sampled DataFrame: \", sampled_train_data.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5302add",
   "metadata": {},
   "source": [
    "# Fit the model #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e80779c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "                    .appName(\"MyApp\") \\\n",
    "                    .config(\"spark.driver.memory\", \"6g\") \\\n",
    "                    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e8bb32c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, text: string]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5f21b1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"App\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "34cc610a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1bf9fab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Sentiment Analysis Optimized\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cc8d28a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_with_sentiment = train_data.withColumn(\n",
    "    \"sentiment_score\",\n",
    "    F.when(F.col(\"review_text\").like(\"%good%\"), 1)\n",
    "    .when(F.col(\"review_text\").like(\"%bad%\"), -1)\n",
    "    .otherwise(0)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "21bc94a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"review_text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\")\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "assembler = VectorAssembler(inputCols=[\"features\", \"sentiment_score\"], outputCol=\"final_features\")\n",
    "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"final_features\")\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, idf, assembler, lr])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5bb6f84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(1, x.size(0), self.hidden_size)\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 5\n",
    "hidden_size = 64\n",
    "output_size = 1\n",
    "\n",
    "# Initialize the RNN\n",
    "model = SimpleRNN(input_size, hidden_size, output_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b91ce9",
   "metadata": {},
   "source": [
    "### Text Preprocessing and Feature Extraction ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "99eccaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"review_text\", outputCol=\"tokens\")\n",
    "hashingTF = HashingTF(inputCol=\"tokens\", outputCol=\"rawFeatures\")\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"tfidf_features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5d9aea",
   "metadata": {},
   "source": [
    "### Vector Assembly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b0d4a3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=[\"tfidf_features\", \"sentiment_score\"], outputCol=\"final_features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf93bc3",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "de60c983",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"final_features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02357b64",
   "metadata": {},
   "source": [
    "### Pipeline Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "28eaa9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, idf, assembler, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5b61022a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Tokenizer:\n",
      "+-----+----+-----+\n",
      "|label|text|words|\n",
      "+-----+----+-----+\n",
      "|    1| foo|[foo]|\n",
      "|    0| bar|[bar]|\n",
      "+-----+----+-----+\n",
      "\n",
      "After HashingTF:\n",
      "+-----+----+-----+--------------------+\n",
      "|label|text|words|            features|\n",
      "+-----+----+-----+--------------------+\n",
      "|    1| foo|[foo]|(262144,[215198],...|\n",
      "|    0| bar|[bar]|(262144,[111892],...|\n",
      "+-----+----+-----+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-22 14:01:29,110 WARN netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "2023-09-22 14:01:29,110 WARN netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Tokenizer, HashingTF\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Sample data\n",
    "train_data = [Row(label=1, text=\"foo\"), Row(label=0, text=\"bar\")]\n",
    "train_df = spark.createDataFrame(train_data)\n",
    "\n",
    "# Define stages\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.01)\n",
    "\n",
    "# Instead of creating a pipeline with all stages, run each stage individually\n",
    "tokenized_data = tokenizer.transform(train_df)\n",
    "hashed_data = hashingTF.transform(tokenized_data)\n",
    "\n",
    "# Check resources and data at each stage\n",
    "print(\"After Tokenizer:\")\n",
    "tokenized_data.show()\n",
    "\n",
    "print(\"After HashingTF:\")\n",
    "hashed_data.show()\n",
    "\n",
    "# Finally, fit the model\n",
    "lr_model = lr.fit(hashed_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "afab05bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: bigint, text: string, words: array<string>, features: vector]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.storagelevel import StorageLevel\n",
    "\n",
    "# Persist intermediate DataFrames to disk\n",
    "tokenized_data.persist(StorageLevel.DISK_ONLY)\n",
    "hashed_data.persist(StorageLevel.DISK_ONLY)\n",
    "\n",
    "# Unpersist the previous DataFrames to free up memory\n",
    "train_df.unpersist()\n",
    "tokenized_data.unpersist()\n",
    "\n",
    "# Run the next stage\n",
    "lr_model = lr.fit(hashed_data)\n",
    "\n",
    "# Unpersist the last DataFrame to free up memory\n",
    "hashed_data.unpersist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbe7144",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "adfc51b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-22 14:01:42,994 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 20.2 MiB\n",
      "2023-09-22 14:02:01,304 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 20.2 MiB\n",
      "2023-09-22 14:02:03,647 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 20.2 MiB\n",
      "2023-09-22 14:02:25,155 ERROR executor.Executor: Exception in task 1.0 in stage 63.0 (TID 248)\n",
      "java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
      "\tat java.lang.AbstractStringBuilder.<init>(AbstractStringBuilder.java:68)\n",
      "\tat java.lang.StringBuilder.<init>(StringBuilder.java:106)\n",
      "\tat java.io.ObjectInputStream$BlockDataInputStream.readUTFBody(ObjectInputStream.java:3561)\n",
      "\tat java.io.ObjectInputStream$BlockDataInputStream.readUTF(ObjectInputStream.java:3377)\n",
      "\tat java.io.ObjectInputStream.readString(ObjectInputStream.java:2049)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1651)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n",
      "\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2119)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1657)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n",
      "2023-09-22 14:02:25,155 ERROR executor.Executor: Exception in task 4.0 in stage 63.0 (TID 251)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.io.ObjectInputStream$HandleTable$HandleList.add(ObjectInputStream.java:4018)\n",
      "\tat java.io.ObjectInputStream$HandleTable.markDependency(ObjectInputStream.java:3837)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2433)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n",
      "\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2119)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1657)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n",
      "2023-09-22 14:02:25,203 ERROR util.SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 1.0 in stage 63.0 (TID 248),5,main]\n",
      "java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
      "\tat java.lang.AbstractStringBuilder.<init>(AbstractStringBuilder.java:68)\n",
      "\tat java.lang.StringBuilder.<init>(StringBuilder.java:106)\n",
      "\tat java.io.ObjectInputStream$BlockDataInputStream.readUTFBody(ObjectInputStream.java:3561)\n",
      "\tat java.io.ObjectInputStream$BlockDataInputStream.readUTF(ObjectInputStream.java:3377)\n",
      "\tat java.io.ObjectInputStream.readString(ObjectInputStream.java:2049)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1651)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n",
      "\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2119)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1657)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n",
      "2023-09-22 14:02:25,212 ERROR util.SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 4.0 in stage 63.0 (TID 251),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.io.ObjectInputStream$HandleTable$HandleList.add(ObjectInputStream.java:4018)\n",
      "\tat java.io.ObjectInputStream$HandleTable.markDependency(ObjectInputStream.java:3837)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2433)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n",
      "\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2119)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1657)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n",
      "2023-09-22 14:02:25,243 WARN scheduler.TaskSetManager: Lost task 4.0 in stage 63.0 (TID 251) (fabio-poli-vm executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.io.ObjectInputStream$HandleTable$HandleList.add(ObjectInputStream.java:4018)\n",
      "\tat java.io.ObjectInputStream$HandleTable.markDependency(ObjectInputStream.java:3837)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2433)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n",
      "\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2119)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1657)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n",
      "\n",
      "2023-09-22 14:02:25,252 ERROR scheduler.TaskSetManager: Task 4 in stage 63.0 failed 1 times; aborting job\n",
      "2023-09-22 14:02:25,274 WARN scheduler.TaskSetManager: Lost task 1.0 in stage 63.0 (TID 248) (fabio-poli-vm executor driver): java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
      "\tat java.lang.AbstractStringBuilder.<init>(AbstractStringBuilder.java:68)\n",
      "\tat java.lang.StringBuilder.<init>(StringBuilder.java:106)\n",
      "\tat java.io.ObjectInputStream$BlockDataInputStream.readUTFBody(ObjectInputStream.java:3561)\n",
      "\tat java.io.ObjectInputStream$BlockDataInputStream.readUTF(ObjectInputStream.java:3377)\n",
      "\tat java.io.ObjectInputStream.readString(ObjectInputStream.java:2049)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1651)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n",
      "\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2119)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1657)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n",
      "\n",
      "2023-09-22 14:02:25,347 WARN storage.BlockManager: Putting block rdd_227_2 failed due to exception org.apache.spark.TaskKilledException.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-22 14:02:25,347 WARN storage.BlockManager: Putting block rdd_227_3 failed due to exception org.apache.spark.TaskKilledException.\n",
      "2023-09-22 14:02:25,356 WARN storage.BlockManager: Block rdd_227_2 could not be removed as it was not found on disk or in memory\n",
      "2023-09-22 14:02:25,350 WARN storage.BlockManager: Putting block rdd_227_5 failed due to exception org.apache.spark.TaskKilledException.\n",
      "2023-09-22 14:02:25,920 WARN storage.BlockManager: Block rdd_227_5 could not be removed as it was not found on disk or in memory\n",
      "2023-09-22 14:02:25,925 WARN storage.BlockManager: Block rdd_227_3 could not be removed as it was not found on disk or in memory\n",
      "2023-09-22 14:02:26,003 WARN storage.BlockManager: Putting block rdd_227_0 failed due to exception org.apache.spark.TaskKilledException.\n",
      "2023-09-22 14:02:26,005 WARN storage.BlockManager: Block rdd_227_0 could not be removed as it was not found on disk or in memory\n",
      "2023-09-22 14:02:26,012 ERROR util.Instrumentation: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 63.0 failed 1 times, most recent failure: Lost task 4.0 in stage 63.0 (TID 251) (fabio-poli-vm executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.io.ObjectInputStream$HandleTable$HandleList.add(ObjectInputStream.java:4018)\n",
      "\tat java.io.ObjectInputStream$HandleTable.markDependency(ObjectInputStream.java:3837)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2433)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n",
      "\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2119)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1657)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2450)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2399)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2398)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2398)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1156)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1156)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1156)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2638)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2580)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2569)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2224)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2319)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$fold$1(RDD.scala:1183)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n",
      "\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1177)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$1(RDD.scala:1246)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n",
      "\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1222)\n",
      "\tat org.apache.spark.ml.optim.loss.RDDLossFunction.calculate(RDDLossFunction.scala:61)\n",
      "\tat org.apache.spark.ml.optim.loss.RDDLossFunction.calculate(RDDLossFunction.scala:47)\n",
      "\tat breeze.optimize.CachedDiffFunction.calculate(CachedDiffFunction.scala:24)\n",
      "\tat breeze.optimize.FirstOrderMinimizer.calculateObjective(FirstOrderMinimizer.scala:50)\n",
      "\tat breeze.optimize.FirstOrderMinimizer.initialState(FirstOrderMinimizer.scala:44)\n",
      "\tat breeze.optimize.FirstOrderMinimizer.iterations(FirstOrderMinimizer.scala:96)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression.trainImpl(LogisticRegression.scala:999)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression.$anonfun$train$1(LogisticRegression.scala:628)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:495)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:286)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:151)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:115)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.io.ObjectInputStream$HandleTable$HandleList.add(ObjectInputStream.java:4018)\n",
      "\tat java.io.ObjectInputStream$HandleTable.markDependency(ObjectInputStream.java:3837)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2433)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n",
      "\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2119)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1657)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-22 14:02:26,120 WARN scheduler.TaskSetManager: Lost task 2.0 in stage 63.0 (TID 249) (fabio-poli-vm executor driver): TaskKilled (Stage cancelled)\n",
      "2023-09-22 14:02:26,154 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 63.0 (TID 247) (fabio-poli-vm executor driver): TaskKilled (Stage cancelled)\n",
      "2023-09-22 14:02:26,214 WARN scheduler.TaskSetManager: Lost task 5.0 in stage 63.0 (TID 252) (fabio-poli-vm executor driver): TaskKilled (Stage cancelled)\n",
      "2023-09-22 14:02:26,214 WARN scheduler.TaskSetManager: Lost task 3.0 in stage 63.0 (TID 250) (fabio-poli-vm executor driver): TaskKilled (Stage cancelled)\n",
      "2023-09-22 14:02:26,893 WARN storage.BlockManager: Putting block rdd_227_7 failed due to exception org.apache.spark.TaskKilledException.\n",
      "2023-09-22 14:02:26,904 WARN storage.BlockManager: Block rdd_227_7 could not be removed as it was not found on disk or in memory\n",
      "2023-09-22 14:02:26,908 WARN scheduler.TaskSetManager: Lost task 7.0 in stage 63.0 (TID 254) (fabio-poli-vm executor driver): TaskKilled (Stage cancelled)\n",
      "2023-09-22 14:02:26,959 WARN storage.BlockManager: Putting block rdd_227_6 failed due to exception org.apache.spark.TaskKilledException.\n",
      "2023-09-22 14:02:26,965 WARN storage.BlockManager: Block rdd_227_6 could not be removed as it was not found on disk or in memory\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o206.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 63.0 failed 1 times, most recent failure: Lost task 4.0 in stage 63.0 (TID 251) (fabio-poli-vm executor driver): java.lang.OutOfMemoryError: Java heap space\n\tat java.io.ObjectInputStream$HandleTable$HandleList.add(ObjectInputStream.java:4018)\n\tat java.io.ObjectInputStream$HandleTable.markDependency(ObjectInputStream.java:3837)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2433)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2119)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1657)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2450)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2399)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2398)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2398)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1156)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1156)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1156)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2638)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2580)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2569)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2224)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2319)\n\tat org.apache.spark.rdd.RDD.$anonfun$fold$1(RDD.scala:1183)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1177)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$1(RDD.scala:1246)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1222)\n\tat org.apache.spark.ml.optim.loss.RDDLossFunction.calculate(RDDLossFunction.scala:61)\n\tat org.apache.spark.ml.optim.loss.RDDLossFunction.calculate(RDDLossFunction.scala:47)\n\tat breeze.optimize.CachedDiffFunction.calculate(CachedDiffFunction.scala:24)\n\tat breeze.optimize.FirstOrderMinimizer.calculateObjective(FirstOrderMinimizer.scala:50)\n\tat breeze.optimize.FirstOrderMinimizer.initialState(FirstOrderMinimizer.scala:44)\n\tat breeze.optimize.FirstOrderMinimizer.iterations(FirstOrderMinimizer.scala:96)\n\tat org.apache.spark.ml.classification.LogisticRegression.trainImpl(LogisticRegression.scala:999)\n\tat org.apache.spark.ml.classification.LogisticRegression.$anonfun$train$1(LogisticRegression.scala:628)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:495)\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:286)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:151)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:115)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n\tat java.io.ObjectInputStream$HandleTable$HandleList.add(ObjectInputStream.java:4018)\n\tat java.io.ObjectInputStream$HandleTable.markDependency(ObjectInputStream.java:3837)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2433)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2119)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1657)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13590/983856174.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampled_train_data_with_sentiment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             raise TypeError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    112\u001b[0m                     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# must be an Estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m                     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m                     \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mindexOfLastEstimator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             raise TypeError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    334\u001b[0m         \"\"\"\n\u001b[1;32m    335\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o206.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 63.0 failed 1 times, most recent failure: Lost task 4.0 in stage 63.0 (TID 251) (fabio-poli-vm executor driver): java.lang.OutOfMemoryError: Java heap space\n\tat java.io.ObjectInputStream$HandleTable$HandleList.add(ObjectInputStream.java:4018)\n\tat java.io.ObjectInputStream$HandleTable.markDependency(ObjectInputStream.java:3837)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2433)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2119)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1657)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2450)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2399)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2398)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2398)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1156)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1156)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1156)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2638)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2580)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2569)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2224)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2319)\n\tat org.apache.spark.rdd.RDD.$anonfun$fold$1(RDD.scala:1183)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1177)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$1(RDD.scala:1246)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1222)\n\tat org.apache.spark.ml.optim.loss.RDDLossFunction.calculate(RDDLossFunction.scala:61)\n\tat org.apache.spark.ml.optim.loss.RDDLossFunction.calculate(RDDLossFunction.scala:47)\n\tat breeze.optimize.CachedDiffFunction.calculate(CachedDiffFunction.scala:24)\n\tat breeze.optimize.FirstOrderMinimizer.calculateObjective(FirstOrderMinimizer.scala:50)\n\tat breeze.optimize.FirstOrderMinimizer.initialState(FirstOrderMinimizer.scala:44)\n\tat breeze.optimize.FirstOrderMinimizer.iterations(FirstOrderMinimizer.scala:96)\n\tat org.apache.spark.ml.classification.LogisticRegression.trainImpl(LogisticRegression.scala:999)\n\tat org.apache.spark.ml.classification.LogisticRegression.$anonfun$train$1(LogisticRegression.scala:628)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:495)\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:286)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:151)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:115)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n\tat java.io.ObjectInputStream$HandleTable$HandleList.add(ObjectInputStream.java:4018)\n\tat java.io.ObjectInputStream$HandleTable.markDependency(ObjectInputStream.java:3837)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2433)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2119)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1657)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)\n"
     ]
    }
   ],
   "source": [
    "model = pipeline.fit(sampled_train_data_with_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e4a2fe",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eded8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383c81c7",
   "metadata": {},
   "source": [
    "### Evaluation Metrics"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d8a2f295",
   "metadata": {},
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Model Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf22ee79",
   "metadata": {},
   "source": [
    "### Overfitting check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b209df44",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.plot(loss , 'b' , label = 'train loss')\n",
    "plt.plot(val_loss , 'orange' , label = 'validation loss')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e40c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "plt.plot(acc , 'b' , label = 'train accuracy')\n",
    "plt.plot(val_acc , 'orange' , label = 'validation accuracy')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
